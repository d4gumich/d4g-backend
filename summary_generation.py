# SUMMARY GENERATION
# @author: Takao Kakegawa

from transformers import BartForConditionalGeneration, BartTokenizer

model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')
tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')

def combine_all_metadata_into_input(text_ranks, themes, locations, disasters):
  '''
    @type text_ranks: list
    @param list of top ranked sentences as string texts. 
    @type themes: list
    @param list of themes as string texts.
    @type locations: list
    @param list of locations as string texts.
    @type disasters: list
    @param list of disasters as string texts.
    @rtype str 
    @rparam string of all the metadata items and top ranked sentences for input into summary pipeline.
    '''
  # combine all metadata features
  input_text_ranks, input_themes, input_locations, input_disasters = "", "", "", ""
  if text_ranks is not None and len(text_ranks) != 0:
    input_text_ranks = (" ").join(text_ranks)
  if themes is not None and len(themes) != 0:
    input_themes = f"The themes discussed in this article are: " + (", ").join(themes) + ". "
  if locations is not None and len(locations) != 0:
    input_locations = f"The primary locations discussed in this article are: " + (", ").join(locations) + ". "
  if disasters is not None and len(disasters) != 0:
    input_disasters = f"The priary disasters discussed in this article are: " + (", ").join(disasters) + ". "

  return input_themes + input_locations + input_disasters + input_text_ranks


def split_text_into_chunks(text, max_tokens=900, overlapPC=5):
  '''
    @type text: str
    @param content text 
    @type max_tokens: int
    @param maximum number of tokens per chunk
    @type overlapPC: int
    @param percent of overlap across consecutive chunks. Range: [0-100]
    @rtype text_chunks: list 
    @rparam list of text chunks 
    '''
  # Split the tokens into pieces with determined amount of overlap:
  tokens = tokenizer.tokenize(text)
  overlap_tokens = int(max_tokens * overlapPC /100)
  chunks = [tokens[i: i + max_tokens] for i in range(0, len(tokens), max_tokens-overlap_tokens)]

  text_chunks = [tokenizer.decode(tokenizer.convert_tokens_to_ids(chunk),skip_special_tokens=True) for chunk in chunks]

  return text_chunks


def summarize(text, maxSummaryLength=500):
  '''
    @type text: str
    @param content text 
    @type maxSummaryLength: int
    @param output summary token length
    @rtype summary: string 
    @rparam output summary generated by Facebook-BART-CNN 
    '''
  # Encode text as tokens and summarize:
  inputs = tokenizer.encode("summarize: " + text,
                            return_tensors='pt',
                            max_length=1024,
                            truncation=True)

  summary_ids = model.generate(inputs,
                               max_length=maxSummaryLength,
                               min_length=int(maxSummaryLength/5),
                               length_penalty=10.0,
                               num_beams=4,
                               early_stopping=True)

  summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

  return summary


def recursive_summarize(text, max_length=1000, recursionLevel=0):
    '''
    @type text: str
    @param content text 
    @type max_length: int
    @param maximum number of tokens of summary
    @type recursionLevel: int
    @param recursion level
    @rtype final_summary: str 
    @rparam string of final summary
    '''
    # Create a summary recursively with input string.
    
    recursionLevel = recursionLevel + 1
    tokens = tokenizer.tokenize(text)
    expectedCountOfChunks = len(tokens)/max_length
    max_length = int(len(tokens)/expectedCountOfChunks) + 2

    # Break the text into pieces of max_length
    pieces = split_text_into_chunks(text, max_tokens=max_length)
    summaries=[]
    k=0
    for k in range(0, len(pieces)):
        piece=pieces[k]
        summary =summarize(piece, maxSummaryLength=max_length/3*2)
        summaries.append(summary)

    concatenated_summary = ' '.join(summaries)

    tokens = tokenizer.tokenize(concatenated_summary)

    if len(tokens) > max_length:
    # If the concatenated_summary is too long, repeat the process
        return recursive_summarize(concatenated_summary,
                                   max_length=max_length,
                                   recursionLevel=recursionLevel)
    else:
    # Concatenate the summaries and summarize again
        final_summary=concatenated_summary
        if len(pieces) > 1:
            final_summary = summarize(concatenated_summary,
                                  maxSummaryLength=max_length)
        return final_summary